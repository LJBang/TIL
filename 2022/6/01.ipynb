{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leejoon-byeong\\anaconda3\\envs\\stdata\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import BartForSequenceClassification, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
       "      <td>씨름의 여자들의 놀이이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
       "      <td>자작극을 벌인 이는 3명이다.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
       "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
       "      <td>원주민들은 종합대책에 만족했다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
       "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            premise  \\\n",
       "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
       "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
       "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
       "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
       "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
       "\n",
       "                                hypothesis          label  \n",
       "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
       "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
       "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
       "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
       "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../5/data/train_data.csv')\n",
    "df_test = pd.read_csv('../5/data/test_data.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leejoon-byeong\\anaconda3\\envs\\stdata\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "c:\\Users\\leejoon-byeong\\anaconda3\\envs\\stdata\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    df['premise'] = df['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', \"\")\n",
    "    df['hypothesis'] = df['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', \"\")\n",
    "    df['premise'] = '<s>' + df['premise'] + '<unused0>'\n",
    "    df['hypothesis'] = df['hypothesis'] + '</s>'\n",
    "    return df\n",
    "\n",
    "df_train = preprocessing(df_train)\n",
    "df_test = preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at gogamza/kobart-base-v2 and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartForSequenceClassification(\n",
      "  (model): BartModel(\n",
      "    (shared): Embedding(30000, 768, padding_idx=3)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
      "      (layers): ModuleList(\n",
      "        (0): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): BartClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v2\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.4.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "MODEL_NAME = 'gogamza/kobart-base-v2'\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "\n",
    "model = BartForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated() / 1024 /1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 14137, 11265, 11821, 22954, 15107, 15509, 10948, 26139, 16116,\n",
      "        16464, 27140, 15033, 15105, 15926, 15581, 14432, 15313,     7, 14137,\n",
      "        11265, 11821, 22954, 15107, 15509, 10948, 26139, 16154, 27140, 15033,\n",
      "        20131, 14270, 19649, 15581, 14432, 15313,     1,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3])\n",
      "<s> 장석영 과학기술정보통신부 제2차관을 비롯한 간담회 참석자들이 기념촬영 하고 있다<unused0> 장석영 과학기술정보통신부 제2차관은 간담회 참석자들과 함께기념촬영 하고 있다</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(df_train, test_size=0.2, shuffle=True, stratify=df_train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kobartDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19998\n",
      "{'input_ids': tensor([    0, 16263, 16954, 15606, 14358, 15015, 15352, 14453, 25156, 21786,\n",
      "        15119, 25492, 22480, 14103, 15173, 15853, 17514, 14518, 16239, 22929,\n",
      "        18809, 24258, 16982,     7, 16954, 15606, 14358, 17514, 14518, 16239,\n",
      "        22929, 18809, 24258, 14242, 14677,  9754,     1,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(1)}\n",
      "<s> 대한민국 이명박 대통령은 이날 조선민주주의인민공화국의 장거리 로켓 발사와 관련해 긴급 국가안전보장회의를 소집했다<unused0> 이명박 대통령은 이날 긴급 국가안전보장회의를 소집하지 않았다</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "processed_train = copy.copy(tokenized_train)\n",
    "processed_train.pop('token_type_ids')\n",
    "processed_eval = copy.copy(tokenized_eval)\n",
    "processed_eval.pop('token_type_ids')\n",
    "\n",
    "train_dataset = kobartDataset(processed_train, train_label)\n",
    "eval_dataset = kobartDataset(processed_eval, eval_label)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(19997))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions[0].argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_strategy = 'no',\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 500/4375 [02:07<17:03,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8158, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 11%|█▏        | 500/4375 [02:40<17:03,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.604905903339386, 'eval_accuracy': 0.7598, 'eval_runtime': 33.0555, 'eval_samples_per_second': 151.261, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1000/4375 [04:49<14:15,  3.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4782, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 23%|██▎       | 1000/4375 [05:06<14:15,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5557742118835449, 'eval_accuracy': 0.7956, 'eval_runtime': 16.888, 'eval_samples_per_second': 296.068, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1500/4375 [07:13<12:11,  3.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.314, 'learning_rate': 3.285714285714286e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 34%|███▍      | 1500/4375 [07:30<12:11,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7383450269699097, 'eval_accuracy': 0.792, 'eval_runtime': 16.897, 'eval_samples_per_second': 295.911, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2000/4375 [09:34<09:39,  4.10it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2006, 'learning_rate': 2.714285714285714e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 46%|████▌     | 2000/4375 [09:51<09:39,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9102541208267212, 'eval_accuracy': 0.7976, 'eval_runtime': 16.33, 'eval_samples_per_second': 306.185, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2500/4375 [11:54<07:33,  4.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1138, 'learning_rate': 2.1428571428571428e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 2500/4375 [12:10<07:33,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9224607348442078, 'eval_accuracy': 0.798, 'eval_runtime': 16.32, 'eval_samples_per_second': 306.373, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 3000/4375 [14:12<05:34,  4.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0575, 'learning_rate': 1.5714285714285715e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 69%|██████▊   | 3000/4375 [14:29<05:34,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2661964893341064, 'eval_accuracy': 0.7994, 'eval_runtime': 16.288, 'eval_samples_per_second': 306.975, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3500/4375 [16:30<03:31,  4.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0378, 'learning_rate': 1e-05, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 3500/4375 [16:46<03:31,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.415041208267212, 'eval_accuracy': 0.7986, 'eval_runtime': 16.264, 'eval_samples_per_second': 307.427, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 4000/4375 [18:48<01:31,  4.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0253, 'learning_rate': 4.285714285714286e-06, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████▏| 4000/4375 [19:04<01:31,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4580540657043457, 'eval_accuracy': 0.8006, 'eval_runtime': 16.303, 'eval_samples_per_second': 306.692, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4375/4375 [20:35<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1235.7925, 'train_samples_per_second': 3.54, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('./result/best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666\n",
      "{'input_ids': tensor([    0, 14784, 20343, 27194, 10047, 14584, 24093, 14200, 14205,  9561,\n",
      "        14304,     7, 14784, 20343,  1700, 10079,  9495, 14380,  9866, 21693,\n",
      "        22717,     1,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "<s> 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다<unused0> 18일 배를 타고 여행을 떠났습니다</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "test_label = label_to_num(df_test['label'].values)\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(df_test['premise']),\n",
    "    list(df_test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "processed_test = copy.copy(tokenized_test)\n",
    "processed_test.pop('token_type_ids')\n",
    "test_dataset = kobartDataset(processed_test, test_label)\n",
    "\n",
    "print(test_dataset.__len__())\n",
    "print(test_dataset.__getitem__(1665))\n",
    "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:03<00:00, 30.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 2, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 2, 2, 1, 2, 0, 1, 0, 1, 0, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 2, 0, 1, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 1, 1, 1, 2, 1, 0, 2, 2, 1, 1, 0, 2, 1, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 0, 1, 2, 2, 2, 0, 0, 2, 0, 0, 0, 1, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 1, 2, 2, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 1, 0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0, 1, 2, 2, 1, 0, 0, 2, 1, 1, 2, 2, 2, 1, 2, 1, 0, 2, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 0, 0, 2, 0, 2, 1, 0, 2, 0, 1, 0, 2, 0, 1, 1, 1, 2, 2, 2, 0, 2, 1, 1, 0, 0, 1, 0, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 0, 1, 0, 1, 2, 1, 1, 2, 0, 2, 0, 0, 1, 1, 1, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 2, 1, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 1, 0, 2, 0, 2, 0, 2, 1, 2, 0, 1, 0, 2, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2, 1, 2, 1, 2, 1, 1, 0, 2, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 1, 2, 1, 0, 0, 1, 1, 0, 2, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 2, 0, 2, 1, 2, 0, 1, 0, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 1, 0, 2, 1, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 1, 2, 1, 2, 0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 0, 1, 1, 0, 1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 2, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 1, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 2, 1, 1, 2, 1, 0, 0, 0, 0, 2, 1, 2, 2, 1, 2, 1, 1, 0, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 0, 0, 2, 2, 0, 2, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 2, 2, 2, 0, 1, 0, 1, 0, 1, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 2, 1, 0, 1, 1, 2, 1, 2, 0, 2, 0, 0, 1, 2, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 0, 2, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 0, 2, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 0, 2, 1, 2, 0, 0, 1, 2, 0, 1, 0, 1, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 2, 2, 1, 1, 0, 2, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 1, 2, 0, 2, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 0, 2, 2, 1, 1, 0, 2, 2, 1, 0, 2, 1, 0, 0, 2, 1, 2, 1, 0, 2, 2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 2, 2, 2, 0, 2, 2, 0, 1, 2, 0, 2, 0, 2, 0, 2, 1, 0, 0, 2, 0, 2, 2, 2, 2, 1, 1, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 2, 2, 2, 0, 1, 1, 1, 2, 1, 2, 2, 0, 1, 1, 2, 1, 2, 1, 0, 2, 2, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 2, 1, 0, 2, 0, 2, 2, 2, 1, 1, 2, 1, 0, 0, 2, 1, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 2, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2, 0, 0, 2, 1, 2, 1, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 1, 2, 0, 1, 0, 0, 2, 1, 2, 1, 2, 2, 1, 2, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 1, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 0, 0, 2, 2, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 0, 1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 2, 0, 1, 2, 2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 0, 0, 2, 2, 1, 2, 1, 2, 2, 0, 2, 0, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 1, 2, 2, 0, 2, 2, 0, 1, 1, 1, 1, 2, 1, 0, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, 1, 0, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 0, 1, 0, 2, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 0, 0, 2, 2, 2, 0, 1, 1, 1, 0, 0, 1, 0, 2, 2, 2, 0, 2, 0, 1, 0, 1, 0, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 0, 2, 1, 1, 0, 2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 0, 2, 0, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 2, 0, 2, 1, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 1, 0, 1, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 1, 2, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 2, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 0, 1, 1, 2, 2, 1, 0, 1, 2, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 2, 2, 1, 1, 2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 2, 1, 1, 0, 2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 0, 0, 2, 1, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 2, 0, 1, 0, 0, 1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2, 2, 0, 0, 2, 2, 1, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 0, 0, 1, 2, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 1, 1, 2, 0, 0, 2, 0, 2, 2, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "print(pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 'contradiction'], [1, 'neutral'], [2, 'neutral'], [3, 'contradiction'], [4, 'contradiction'], [5, 'contradiction'], [6, 'neutral'], [7, 'entailment'], [8, 'entailment'], [9, 'entailment'], [10, 'contradiction'], [11, 'entailment'], [12, 'contradiction'], [13, 'entailment'], [14, 'neutral'], [15, 'neutral'], [16, 'entailment'], [17, 'neutral'], [18, 'contradiction'], [19, 'neutral'], [20, 'neutral'], [21, 'contradiction'], [22, 'contradiction'], [23, 'entailment'], [24, 'contradiction'], [25, 'contradiction'], [26, 'contradiction'], [27, 'neutral'], [28, 'neutral'], [29, 'neutral'], [30, 'contradiction'], [31, 'entailment'], [32, 'contradiction'], [33, 'neutral'], [34, 'neutral'], [35, 'contradiction'], [36, 'entailment'], [37, 'contradiction'], [38, 'entailment'], [39, 'contradiction'], [40, 'neutral'], [41, 'entailment'], [42, 'contradiction'], [43, 'neutral'], [44, 'neutral'], [45, 'contradiction'], [46, 'neutral'], [47, 'entailment'], [48, 'contradiction'], [49, 'entailment'], [50, 'contradiction'], [51, 'entailment'], [52, 'contradiction'], [53, 'neutral'], [54, 'entailment'], [55, 'neutral'], [56, 'neutral'], [57, 'neutral'], [58, 'neutral'], [59, 'neutral'], [60, 'entailment'], [61, 'contradiction'], [62, 'neutral'], [63, 'neutral'], [64, 'neutral'], [65, 'contradiction'], [66, 'neutral'], [67, 'neutral'], [68, 'neutral'], [69, 'contradiction'], [70, 'entailment'], [71, 'neutral'], [72, 'entailment'], [73, 'contradiction'], [74, 'neutral'], [75, 'neutral'], [76, 'neutral'], [77, 'neutral'], [78, 'neutral'], [79, 'neutral'], [80, 'neutral'], [81, 'neutral'], [82, 'contradiction'], [83, 'contradiction'], [84, 'entailment'], [85, 'entailment'], [86, 'neutral'], [87, 'contradiction'], [88, 'entailment'], [89, 'neutral'], [90, 'contradiction'], [91, 'neutral'], [92, 'entailment'], [93, 'contradiction'], [94, 'entailment'], [95, 'neutral'], [96, 'entailment'], [97, 'contradiction'], [98, 'entailment'], [99, 'neutral'], [100, 'neutral'], [101, 'entailment'], [102, 'neutral'], [103, 'entailment'], [104, 'neutral'], [105, 'entailment'], [106, 'neutral'], [107, 'neutral'], [108, 'contradiction'], [109, 'contradiction'], [110, 'contradiction'], [111, 'neutral'], [112, 'contradiction'], [113, 'entailment'], [114, 'neutral'], [115, 'neutral'], [116, 'contradiction'], [117, 'contradiction'], [118, 'entailment'], [119, 'neutral'], [120, 'contradiction'], [121, 'neutral'], [122, 'contradiction'], [123, 'neutral'], [124, 'entailment'], [125, 'entailment'], [126, 'entailment'], [127, 'entailment'], [128, 'contradiction'], [129, 'neutral'], [130, 'contradiction'], [131, 'neutral'], [132, 'entailment'], [133, 'entailment'], [134, 'neutral'], [135, 'contradiction'], [136, 'neutral'], [137, 'neutral'], [138, 'contradiction'], [139, 'neutral'], [140, 'neutral'], [141, 'neutral'], [142, 'contradiction'], [143, 'entailment'], [144, 'neutral'], [145, 'neutral'], [146, 'entailment'], [147, 'contradiction'], [148, 'neutral'], [149, 'neutral'], [150, 'neutral'], [151, 'entailment'], [152, 'entailment'], [153, 'neutral'], [154, 'entailment'], [155, 'entailment'], [156, 'entailment'], [157, 'contradiction'], [158, 'contradiction'], [159, 'neutral'], [160, 'neutral'], [161, 'entailment'], [162, 'neutral'], [163, 'entailment'], [164, 'neutral'], [165, 'contradiction'], [166, 'neutral'], [167, 'entailment'], [168, 'neutral'], [169, 'entailment'], [170, 'neutral'], [171, 'neutral'], [172, 'contradiction'], [173, 'neutral'], [174, 'contradiction'], [175, 'neutral'], [176, 'neutral'], [177, 'neutral'], [178, 'contradiction'], [179, 'neutral'], [180, 'neutral'], [181, 'neutral'], [182, 'contradiction'], [183, 'neutral'], [184, 'neutral'], [185, 'contradiction'], [186, 'contradiction'], [187, 'neutral'], [188, 'entailment'], [189, 'neutral'], [190, 'neutral'], [191, 'entailment'], [192, 'contradiction'], [193, 'neutral'], [194, 'neutral'], [195, 'entailment'], [196, 'contradiction'], [197, 'neutral'], [198, 'contradiction'], [199, 'neutral'], [200, 'neutral'], [201, 'neutral'], [202, 'entailment'], [203, 'contradiction'], [204, 'contradiction'], [205, 'entailment'], [206, 'neutral'], [207, 'neutral'], [208, 'entailment'], [209, 'neutral'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'neutral'], [214, 'contradiction'], [215, 'contradiction'], [216, 'neutral'], [217, 'entailment'], [218, 'contradiction'], [219, 'contradiction'], [220, 'neutral'], [221, 'contradiction'], [222, 'entailment'], [223, 'contradiction'], [224, 'neutral'], [225, 'entailment'], [226, 'neutral'], [227, 'neutral'], [228, 'contradiction'], [229, 'contradiction'], [230, 'entailment'], [231, 'neutral'], [232, 'contradiction'], [233, 'entailment'], [234, 'entailment'], [235, 'neutral'], [236, 'neutral'], [237, 'neutral'], [238, 'entailment'], [239, 'entailment'], [240, 'entailment'], [241, 'entailment'], [242, 'neutral'], [243, 'neutral'], [244, 'neutral'], [245, 'entailment'], [246, 'contradiction'], [247, 'neutral'], [248, 'neutral'], [249, 'contradiction'], [250, 'entailment'], [251, 'contradiction'], [252, 'neutral'], [253, 'entailment'], [254, 'contradiction'], [255, 'neutral'], [256, 'neutral'], [257, 'contradiction'], [258, 'entailment'], [259, 'entailment'], [260, 'neutral'], [261, 'contradiction'], [262, 'contradiction'], [263, 'neutral'], [264, 'neutral'], [265, 'neutral'], [266, 'contradiction'], [267, 'neutral'], [268, 'contradiction'], [269, 'entailment'], [270, 'neutral'], [271, 'neutral'], [272, 'entailment'], [273, 'entailment'], [274, 'neutral'], [275, 'contradiction'], [276, 'contradiction'], [277, 'contradiction'], [278, 'neutral'], [279, 'neutral'], [280, 'neutral'], [281, 'entailment'], [282, 'entailment'], [283, 'entailment'], [284, 'neutral'], [285, 'entailment'], [286, 'neutral'], [287, 'contradiction'], [288, 'entailment'], [289, 'neutral'], [290, 'entailment'], [291, 'contradiction'], [292, 'entailment'], [293, 'neutral'], [294, 'entailment'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'neutral'], [299, 'neutral'], [300, 'neutral'], [301, 'entailment'], [302, 'neutral'], [303, 'contradiction'], [304, 'contradiction'], [305, 'entailment'], [306, 'entailment'], [307, 'contradiction'], [308, 'entailment'], [309, 'entailment'], [310, 'neutral'], [311, 'contradiction'], [312, 'contradiction'], [313, 'neutral'], [314, 'neutral'], [315, 'neutral'], [316, 'neutral'], [317, 'neutral'], [318, 'entailment'], [319, 'entailment'], [320, 'entailment'], [321, 'neutral'], [322, 'neutral'], [323, 'entailment'], [324, 'contradiction'], [325, 'entailment'], [326, 'contradiction'], [327, 'neutral'], [328, 'contradiction'], [329, 'contradiction'], [330, 'neutral'], [331, 'entailment'], [332, 'neutral'], [333, 'entailment'], [334, 'entailment'], [335, 'contradiction'], [336, 'contradiction'], [337, 'contradiction'], [338, 'contradiction'], [339, 'entailment'], [340, 'contradiction'], [341, 'neutral'], [342, 'neutral'], [343, 'contradiction'], [344, 'neutral'], [345, 'entailment'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'neutral'], [350, 'contradiction'], [351, 'neutral'], [352, 'contradiction'], [353, 'neutral'], [354, 'neutral'], [355, 'neutral'], [356, 'neutral'], [357, 'entailment'], [358, 'contradiction'], [359, 'neutral'], [360, 'contradiction'], [361, 'contradiction'], [362, 'neutral'], [363, 'neutral'], [364, 'contradiction'], [365, 'neutral'], [366, 'neutral'], [367, 'entailment'], [368, 'entailment'], [369, 'entailment'], [370, 'contradiction'], [371, 'neutral'], [372, 'entailment'], [373, 'neutral'], [374, 'neutral'], [375, 'contradiction'], [376, 'entailment'], [377, 'neutral'], [378, 'entailment'], [379, 'neutral'], [380, 'entailment'], [381, 'neutral'], [382, 'contradiction'], [383, 'neutral'], [384, 'entailment'], [385, 'contradiction'], [386, 'entailment'], [387, 'neutral'], [388, 'neutral'], [389, 'contradiction'], [390, 'neutral'], [391, 'entailment'], [392, 'contradiction'], [393, 'contradiction'], [394, 'contradiction'], [395, 'contradiction'], [396, 'entailment'], [397, 'neutral'], [398, 'contradiction'], [399, 'neutral'], [400, 'contradiction'], [401, 'neutral'], [402, 'contradiction'], [403, 'contradiction'], [404, 'entailment'], [405, 'neutral'], [406, 'entailment'], [407, 'neutral'], [408, 'contradiction'], [409, 'neutral'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'neutral'], [415, 'contradiction'], [416, 'contradiction'], [417, 'contradiction'], [418, 'contradiction'], [419, 'contradiction'], [420, 'neutral'], [421, 'neutral'], [422, 'neutral'], [423, 'neutral'], [424, 'entailment'], [425, 'neutral'], [426, 'neutral'], [427, 'contradiction'], [428, 'contradiction'], [429, 'neutral'], [430, 'entailment'], [431, 'neutral'], [432, 'neutral'], [433, 'neutral'], [434, 'contradiction'], [435, 'neutral'], [436, 'entailment'], [437, 'entailment'], [438, 'entailment'], [439, 'entailment'], [440, 'entailment'], [441, 'entailment'], [442, 'contradiction'], [443, 'contradiction'], [444, 'entailment'], [445, 'neutral'], [446, 'neutral'], [447, 'entailment'], [448, 'contradiction'], [449, 'neutral'], [450, 'contradiction'], [451, 'entailment'], [452, 'entailment'], [453, 'contradiction'], [454, 'contradiction'], [455, 'entailment'], [456, 'neutral'], [457, 'contradiction'], [458, 'neutral'], [459, 'contradiction'], [460, 'entailment'], [461, 'entailment'], [462, 'contradiction'], [463, 'contradiction'], [464, 'entailment'], [465, 'contradiction'], [466, 'contradiction'], [467, 'neutral'], [468, 'entailment'], [469, 'neutral'], [470, 'contradiction'], [471, 'neutral'], [472, 'entailment'], [473, 'contradiction'], [474, 'entailment'], [475, 'neutral'], [476, 'entailment'], [477, 'neutral'], [478, 'neutral'], [479, 'contradiction'], [480, 'neutral'], [481, 'entailment'], [482, 'entailment'], [483, 'contradiction'], [484, 'neutral'], [485, 'contradiction'], [486, 'entailment'], [487, 'neutral'], [488, 'contradiction'], [489, 'entailment'], [490, 'neutral'], [491, 'entailment'], [492, 'neutral'], [493, 'entailment'], [494, 'entailment'], [495, 'entailment'], [496, 'entailment'], [497, 'neutral'], [498, 'neutral'], [499, 'neutral'], [500, 'neutral'], [501, 'contradiction'], [502, 'neutral'], [503, 'contradiction'], [504, 'neutral'], [505, 'entailment'], [506, 'neutral'], [507, 'contradiction'], [508, 'entailment'], [509, 'neutral'], [510, 'contradiction'], [511, 'entailment'], [512, 'entailment'], [513, 'entailment'], [514, 'entailment'], [515, 'contradiction'], [516, 'contradiction'], [517, 'entailment'], [518, 'contradiction'], [519, 'contradiction'], [520, 'neutral'], [521, 'neutral'], [522, 'contradiction'], [523, 'neutral'], [524, 'neutral'], [525, 'neutral'], [526, 'contradiction'], [527, 'contradiction'], [528, 'contradiction'], [529, 'entailment'], [530, 'contradiction'], [531, 'contradiction'], [532, 'contradiction'], [533, 'neutral'], [534, 'neutral'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'neutral'], [539, 'neutral'], [540, 'entailment'], [541, 'contradiction'], [542, 'contradiction'], [543, 'entailment'], [544, 'contradiction'], [545, 'neutral'], [546, 'contradiction'], [547, 'entailment'], [548, 'neutral'], [549, 'entailment'], [550, 'neutral'], [551, 'entailment'], [552, 'contradiction'], [553, 'entailment'], [554, 'entailment'], [555, 'neutral'], [556, 'entailment'], [557, 'neutral'], [558, 'neutral'], [559, 'entailment'], [560, 'contradiction'], [561, 'contradiction'], [562, 'neutral'], [563, 'neutral'], [564, 'entailment'], [565, 'neutral'], [566, 'entailment'], [567, 'entailment'], [568, 'neutral'], [569, 'contradiction'], [570, 'entailment'], [571, 'entailment'], [572, 'entailment'], [573, 'contradiction'], [574, 'entailment'], [575, 'contradiction'], [576, 'entailment'], [577, 'neutral'], [578, 'entailment'], [579, 'neutral'], [580, 'entailment'], [581, 'entailment'], [582, 'entailment'], [583, 'neutral'], [584, 'entailment'], [585, 'entailment'], [586, 'neutral'], [587, 'entailment'], [588, 'entailment'], [589, 'entailment'], [590, 'entailment'], [591, 'neutral'], [592, 'contradiction'], [593, 'entailment'], [594, 'contradiction'], [595, 'contradiction'], [596, 'neutral'], [597, 'neutral'], [598, 'entailment'], [599, 'entailment'], [600, 'contradiction'], [601, 'neutral'], [602, 'contradiction'], [603, 'entailment'], [604, 'neutral'], [605, 'neutral'], [606, 'contradiction'], [607, 'contradiction'], [608, 'neutral'], [609, 'contradiction'], [610, 'entailment'], [611, 'entailment'], [612, 'entailment'], [613, 'entailment'], [614, 'neutral'], [615, 'contradiction'], [616, 'neutral'], [617, 'neutral'], [618, 'contradiction'], [619, 'neutral'], [620, 'contradiction'], [621, 'contradiction'], [622, 'entailment'], [623, 'contradiction'], [624, 'entailment'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'neutral'], [629, 'entailment'], [630, 'entailment'], [631, 'contradiction'], [632, 'neutral'], [633, 'entailment'], [634, 'neutral'], [635, 'contradiction'], [636, 'contradiction'], [637, 'entailment'], [638, 'entailment'], [639, 'neutral'], [640, 'neutral'], [641, 'entailment'], [642, 'neutral'], [643, 'entailment'], [644, 'neutral'], [645, 'contradiction'], [646, 'neutral'], [647, 'contradiction'], [648, 'contradiction'], [649, 'neutral'], [650, 'entailment'], [651, 'neutral'], [652, 'contradiction'], [653, 'neutral'], [654, 'neutral'], [655, 'neutral'], [656, 'entailment'], [657, 'contradiction'], [658, 'entailment'], [659, 'contradiction'], [660, 'entailment'], [661, 'contradiction'], [662, 'neutral'], [663, 'entailment'], [664, 'neutral'], [665, 'contradiction'], [666, 'neutral'], [667, 'neutral'], [668, 'neutral'], [669, 'neutral'], [670, 'neutral'], [671, 'neutral'], [672, 'neutral'], [673, 'contradiction'], [674, 'neutral'], [675, 'neutral'], [676, 'neutral'], [677, 'neutral'], [678, 'contradiction'], [679, 'contradiction'], [680, 'neutral'], [681, 'neutral'], [682, 'neutral'], [683, 'contradiction'], [684, 'neutral'], [685, 'contradiction'], [686, 'neutral'], [687, 'neutral'], [688, 'entailment'], [689, 'entailment'], [690, 'neutral'], [691, 'entailment'], [692, 'entailment'], [693, 'neutral'], [694, 'entailment'], [695, 'neutral'], [696, 'entailment'], [697, 'neutral'], [698, 'entailment'], [699, 'entailment'], [700, 'neutral'], [701, 'entailment'], [702, 'entailment'], [703, 'neutral'], [704, 'contradiction'], [705, 'entailment'], [706, 'contradiction'], [707, 'contradiction'], [708, 'neutral'], [709, 'contradiction'], [710, 'neutral'], [711, 'entailment'], [712, 'neutral'], [713, 'entailment'], [714, 'entailment'], [715, 'contradiction'], [716, 'neutral'], [717, 'neutral'], [718, 'neutral'], [719, 'entailment'], [720, 'neutral'], [721, 'neutral'], [722, 'neutral'], [723, 'contradiction'], [724, 'neutral'], [725, 'contradiction'], [726, 'contradiction'], [727, 'contradiction'], [728, 'contradiction'], [729, 'entailment'], [730, 'entailment'], [731, 'neutral'], [732, 'entailment'], [733, 'neutral'], [734, 'entailment'], [735, 'neutral'], [736, 'entailment'], [737, 'neutral'], [738, 'neutral'], [739, 'entailment'], [740, 'neutral'], [741, 'entailment'], [742, 'entailment'], [743, 'contradiction'], [744, 'contradiction'], [745, 'neutral'], [746, 'neutral'], [747, 'contradiction'], [748, 'neutral'], [749, 'contradiction'], [750, 'neutral'], [751, 'neutral'], [752, 'neutral'], [753, 'neutral'], [754, 'entailment'], [755, 'neutral'], [756, 'contradiction'], [757, 'contradiction'], [758, 'neutral'], [759, 'neutral'], [760, 'entailment'], [761, 'neutral'], [762, 'entailment'], [763, 'contradiction'], [764, 'neutral'], [765, 'entailment'], [766, 'neutral'], [767, 'entailment'], [768, 'entailment'], [769, 'neutral'], [770, 'contradiction'], [771, 'neutral'], [772, 'entailment'], [773, 'entailment'], [774, 'neutral'], [775, 'neutral'], [776, 'contradiction'], [777, 'neutral'], [778, 'neutral'], [779, 'neutral'], [780, 'entailment'], [781, 'neutral'], [782, 'contradiction'], [783, 'neutral'], [784, 'entailment'], [785, 'entailment'], [786, 'contradiction'], [787, 'neutral'], [788, 'entailment'], [789, 'contradiction'], [790, 'entailment'], [791, 'contradiction'], [792, 'entailment'], [793, 'neutral'], [794, 'neutral'], [795, 'entailment'], [796, 'entailment'], [797, 'entailment'], [798, 'entailment'], [799, 'neutral'], [800, 'entailment'], [801, 'contradiction'], [802, 'neutral'], [803, 'neutral'], [804, 'contradiction'], [805, 'contradiction'], [806, 'entailment'], [807, 'neutral'], [808, 'contradiction'], [809, 'contradiction'], [810, 'entailment'], [811, 'contradiction'], [812, 'neutral'], [813, 'neutral'], [814, 'neutral'], [815, 'neutral'], [816, 'neutral'], [817, 'neutral'], [818, 'neutral'], [819, 'entailment'], [820, 'neutral'], [821, 'contradiction'], [822, 'contradiction'], [823, 'neutral'], [824, 'entailment'], [825, 'neutral'], [826, 'contradiction'], [827, 'neutral'], [828, 'contradiction'], [829, 'contradiction'], [830, 'entailment'], [831, 'contradiction'], [832, 'contradiction'], [833, 'entailment'], [834, 'neutral'], [835, 'contradiction'], [836, 'entailment'], [837, 'contradiction'], [838, 'neutral'], [839, 'contradiction'], [840, 'contradiction'], [841, 'entailment'], [842, 'contradiction'], [843, 'entailment'], [844, 'neutral'], [845, 'neutral'], [846, 'contradiction'], [847, 'contradiction'], [848, 'entailment'], [849, 'neutral'], [850, 'neutral'], [851, 'contradiction'], [852, 'entailment'], [853, 'neutral'], [854, 'contradiction'], [855, 'entailment'], [856, 'entailment'], [857, 'neutral'], [858, 'contradiction'], [859, 'neutral'], [860, 'contradiction'], [861, 'entailment'], [862, 'neutral'], [863, 'neutral'], [864, 'entailment'], [865, 'entailment'], [866, 'entailment'], [867, 'neutral'], [868, 'contradiction'], [869, 'neutral'], [870, 'contradiction'], [871, 'neutral'], [872, 'entailment'], [873, 'neutral'], [874, 'neutral'], [875, 'contradiction'], [876, 'entailment'], [877, 'neutral'], [878, 'neutral'], [879, 'neutral'], [880, 'neutral'], [881, 'neutral'], [882, 'entailment'], [883, 'entailment'], [884, 'entailment'], [885, 'contradiction'], [886, 'neutral'], [887, 'neutral'], [888, 'entailment'], [889, 'neutral'], [890, 'entailment'], [891, 'contradiction'], [892, 'entailment'], [893, 'entailment'], [894, 'contradiction'], [895, 'neutral'], [896, 'entailment'], [897, 'neutral'], [898, 'entailment'], [899, 'entailment'], [900, 'entailment'], [901, 'contradiction'], [902, 'entailment'], [903, 'entailment'], [904, 'entailment'], [905, 'neutral'], [906, 'entailment'], [907, 'contradiction'], [908, 'entailment'], [909, 'entailment'], [910, 'neutral'], [911, 'neutral'], [912, 'neutral'], [913, 'entailment'], [914, 'neutral'], [915, 'neutral'], [916, 'entailment'], [917, 'contradiction'], [918, 'neutral'], [919, 'entailment'], [920, 'neutral'], [921, 'entailment'], [922, 'neutral'], [923, 'entailment'], [924, 'neutral'], [925, 'contradiction'], [926, 'entailment'], [927, 'entailment'], [928, 'neutral'], [929, 'entailment'], [930, 'neutral'], [931, 'neutral'], [932, 'neutral'], [933, 'neutral'], [934, 'contradiction'], [935, 'contradiction'], [936, 'entailment'], [937, 'neutral'], [938, 'entailment'], [939, 'neutral'], [940, 'entailment'], [941, 'neutral'], [942, 'contradiction'], [943, 'contradiction'], [944, 'neutral'], [945, 'neutral'], [946, 'neutral'], [947, 'neutral'], [948, 'neutral'], [949, 'entailment'], [950, 'contradiction'], [951, 'contradiction'], [952, 'contradiction'], [953, 'neutral'], [954, 'contradiction'], [955, 'neutral'], [956, 'neutral'], [957, 'entailment'], [958, 'contradiction'], [959, 'contradiction'], [960, 'neutral'], [961, 'contradiction'], [962, 'neutral'], [963, 'contradiction'], [964, 'entailment'], [965, 'neutral'], [966, 'neutral'], [967, 'entailment'], [968, 'entailment'], [969, 'contradiction'], [970, 'entailment'], [971, 'contradiction'], [972, 'neutral'], [973, 'contradiction'], [974, 'entailment'], [975, 'entailment'], [976, 'entailment'], [977, 'neutral'], [978, 'neutral'], [979, 'contradiction'], [980, 'entailment'], [981, 'neutral'], [982, 'entailment'], [983, 'neutral'], [984, 'neutral'], [985, 'neutral'], [986, 'contradiction'], [987, 'contradiction'], [988, 'neutral'], [989, 'contradiction'], [990, 'entailment'], [991, 'entailment'], [992, 'neutral'], [993, 'contradiction'], [994, 'neutral'], [995, 'entailment'], [996, 'neutral'], [997, 'contradiction'], [998, 'entailment'], [999, 'entailment'], [1000, 'entailment'], [1001, 'entailment'], [1002, 'entailment'], [1003, 'entailment'], [1004, 'neutral'], [1005, 'entailment'], [1006, 'entailment'], [1007, 'contradiction'], [1008, 'neutral'], [1009, 'neutral'], [1010, 'contradiction'], [1011, 'contradiction'], [1012, 'neutral'], [1013, 'contradiction'], [1014, 'neutral'], [1015, 'entailment'], [1016, 'entailment'], [1017, 'entailment'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'entailment'], [1021, 'neutral'], [1022, 'entailment'], [1023, 'entailment'], [1024, 'entailment'], [1025, 'neutral'], [1026, 'neutral'], [1027, 'neutral'], [1028, 'contradiction'], [1029, 'entailment'], [1030, 'entailment'], [1031, 'neutral'], [1032, 'entailment'], [1033, 'entailment'], [1034, 'neutral'], [1035, 'contradiction'], [1036, 'neutral'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'entailment'], [1041, 'neutral'], [1042, 'contradiction'], [1043, 'entailment'], [1044, 'neutral'], [1045, 'neutral'], [1046, 'neutral'], [1047, 'contradiction'], [1048, 'contradiction'], [1049, 'neutral'], [1050, 'entailment'], [1051, 'contradiction'], [1052, 'entailment'], [1053, 'entailment'], [1054, 'neutral'], [1055, 'contradiction'], [1056, 'neutral'], [1057, 'contradiction'], [1058, 'neutral'], [1059, 'neutral'], [1060, 'contradiction'], [1061, 'neutral'], [1062, 'contradiction'], [1063, 'entailment'], [1064, 'entailment'], [1065, 'neutral'], [1066, 'entailment'], [1067, 'entailment'], [1068, 'entailment'], [1069, 'neutral'], [1070, 'entailment'], [1071, 'entailment'], [1072, 'neutral'], [1073, 'neutral'], [1074, 'entailment'], [1075, 'contradiction'], [1076, 'contradiction'], [1077, 'neutral'], [1078, 'contradiction'], [1079, 'entailment'], [1080, 'neutral'], [1081, 'contradiction'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'contradiction'], [1085, 'contradiction'], [1086, 'contradiction'], [1087, 'neutral'], [1088, 'contradiction'], [1089, 'neutral'], [1090, 'entailment'], [1091, 'neutral'], [1092, 'entailment'], [1093, 'neutral'], [1094, 'neutral'], [1095, 'neutral'], [1096, 'entailment'], [1097, 'neutral'], [1098, 'neutral'], [1099, 'contradiction'], [1100, 'entailment'], [1101, 'contradiction'], [1102, 'contradiction'], [1103, 'contradiction'], [1104, 'neutral'], [1105, 'contradiction'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'contradiction'], [1109, 'entailment'], [1110, 'entailment'], [1111, 'contradiction'], [1112, 'entailment'], [1113, 'entailment'], [1114, 'neutral'], [1115, 'neutral'], [1116, 'neutral'], [1117, 'contradiction'], [1118, 'entailment'], [1119, 'contradiction'], [1120, 'entailment'], [1121, 'contradiction'], [1122, 'neutral'], [1123, 'contradiction'], [1124, 'neutral'], [1125, 'contradiction'], [1126, 'entailment'], [1127, 'entailment'], [1128, 'contradiction'], [1129, 'neutral'], [1130, 'entailment'], [1131, 'entailment'], [1132, 'neutral'], [1133, 'contradiction'], [1134, 'entailment'], [1135, 'contradiction'], [1136, 'entailment'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'entailment'], [1140, 'contradiction'], [1141, 'neutral'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'entailment'], [1145, 'neutral'], [1146, 'entailment'], [1147, 'contradiction'], [1148, 'entailment'], [1149, 'contradiction'], [1150, 'entailment'], [1151, 'contradiction'], [1152, 'neutral'], [1153, 'entailment'], [1154, 'contradiction'], [1155, 'neutral'], [1156, 'neutral'], [1157, 'entailment'], [1158, 'neutral'], [1159, 'neutral'], [1160, 'entailment'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'neutral'], [1164, 'neutral'], [1165, 'entailment'], [1166, 'entailment'], [1167, 'entailment'], [1168, 'neutral'], [1169, 'neutral'], [1170, 'contradiction'], [1171, 'neutral'], [1172, 'contradiction'], [1173, 'neutral'], [1174, 'neutral'], [1175, 'entailment'], [1176, 'neutral'], [1177, 'entailment'], [1178, 'neutral'], [1179, 'contradiction'], [1180, 'neutral'], [1181, 'entailment'], [1182, 'neutral'], [1183, 'neutral'], [1184, 'neutral'], [1185, 'neutral'], [1186, 'neutral'], [1187, 'neutral'], [1188, 'neutral'], [1189, 'contradiction'], [1190, 'entailment'], [1191, 'entailment'], [1192, 'entailment'], [1193, 'neutral'], [1194, 'entailment'], [1195, 'entailment'], [1196, 'entailment'], [1197, 'neutral'], [1198, 'contradiction'], [1199, 'contradiction'], [1200, 'neutral'], [1201, 'neutral'], [1202, 'entailment'], [1203, 'neutral'], [1204, 'neutral'], [1205, 'entailment'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'contradiction'], [1209, 'contradiction'], [1210, 'neutral'], [1211, 'contradiction'], [1212, 'entailment'], [1213, 'contradiction'], [1214, 'neutral'], [1215, 'neutral'], [1216, 'neutral'], [1217, 'neutral'], [1218, 'neutral'], [1219, 'neutral'], [1220, 'neutral'], [1221, 'contradiction'], [1222, 'neutral'], [1223, 'neutral'], [1224, 'entailment'], [1225, 'entailment'], [1226, 'contradiction'], [1227, 'entailment'], [1228, 'contradiction'], [1229, 'neutral'], [1230, 'contradiction'], [1231, 'contradiction'], [1232, 'entailment'], [1233, 'neutral'], [1234, 'contradiction'], [1235, 'neutral'], [1236, 'neutral'], [1237, 'entailment'], [1238, 'neutral'], [1239, 'neutral'], [1240, 'neutral'], [1241, 'neutral'], [1242, 'neutral'], [1243, 'neutral'], [1244, 'entailment'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'entailment'], [1248, 'contradiction'], [1249, 'entailment'], [1250, 'neutral'], [1251, 'entailment'], [1252, 'entailment'], [1253, 'contradiction'], [1254, 'neutral'], [1255, 'contradiction'], [1256, 'entailment'], [1257, 'entailment'], [1258, 'contradiction'], [1259, 'entailment'], [1260, 'neutral'], [1261, 'entailment'], [1262, 'entailment'], [1263, 'neutral'], [1264, 'neutral'], [1265, 'neutral'], [1266, 'entailment'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'entailment'], [1271, 'entailment'], [1272, 'contradiction'], [1273, 'entailment'], [1274, 'neutral'], [1275, 'neutral'], [1276, 'neutral'], [1277, 'entailment'], [1278, 'neutral'], [1279, 'entailment'], [1280, 'contradiction'], [1281, 'entailment'], [1282, 'contradiction'], [1283, 'entailment'], [1284, 'neutral'], [1285, 'entailment'], [1286, 'neutral'], [1287, 'neutral'], [1288, 'entailment'], [1289, 'contradiction'], [1290, 'neutral'], [1291, 'neutral'], [1292, 'neutral'], [1293, 'neutral'], [1294, 'contradiction'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'neutral'], [1298, 'contradiction'], [1299, 'neutral'], [1300, 'contradiction'], [1301, 'entailment'], [1302, 'entailment'], [1303, 'neutral'], [1304, 'neutral'], [1305, 'neutral'], [1306, 'contradiction'], [1307, 'neutral'], [1308, 'contradiction'], [1309, 'contradiction'], [1310, 'neutral'], [1311, 'contradiction'], [1312, 'contradiction'], [1313, 'contradiction'], [1314, 'contradiction'], [1315, 'neutral'], [1316, 'neutral'], [1317, 'entailment'], [1318, 'neutral'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'entailment'], [1322, 'neutral'], [1323, 'contradiction'], [1324, 'contradiction'], [1325, 'neutral'], [1326, 'entailment'], [1327, 'neutral'], [1328, 'neutral'], [1329, 'neutral'], [1330, 'neutral'], [1331, 'neutral'], [1332, 'contradiction'], [1333, 'neutral'], [1334, 'contradiction'], [1335, 'neutral'], [1336, 'contradiction'], [1337, 'neutral'], [1338, 'entailment'], [1339, 'neutral'], [1340, 'entailment'], [1341, 'neutral'], [1342, 'entailment'], [1343, 'neutral'], [1344, 'entailment'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'contradiction'], [1348, 'contradiction'], [1349, 'neutral'], [1350, 'neutral'], [1351, 'neutral'], [1352, 'neutral'], [1353, 'contradiction'], [1354, 'neutral'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'entailment'], [1359, 'entailment'], [1360, 'neutral'], [1361, 'contradiction'], [1362, 'neutral'], [1363, 'neutral'], [1364, 'entailment'], [1365, 'neutral'], [1366, 'contradiction'], [1367, 'contradiction'], [1368, 'entailment'], [1369, 'entailment'], [1370, 'neutral'], [1371, 'contradiction'], [1372, 'neutral'], [1373, 'contradiction'], [1374, 'contradiction'], [1375, 'neutral'], [1376, 'neutral'], [1377, 'contradiction'], [1378, 'entailment'], [1379, 'entailment'], [1380, 'neutral'], [1381, 'contradiction'], [1382, 'contradiction'], [1383, 'entailment'], [1384, 'contradiction'], [1385, 'entailment'], [1386, 'entailment'], [1387, 'contradiction'], [1388, 'neutral'], [1389, 'contradiction'], [1390, 'contradiction'], [1391, 'contradiction'], [1392, 'neutral'], [1393, 'neutral'], [1394, 'entailment'], [1395, 'neutral'], [1396, 'entailment'], [1397, 'entailment'], [1398, 'entailment'], [1399, 'neutral'], [1400, 'neutral'], [1401, 'neutral'], [1402, 'entailment'], [1403, 'entailment'], [1404, 'entailment'], [1405, 'contradiction'], [1406, 'entailment'], [1407, 'neutral'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'contradiction'], [1411, 'neutral'], [1412, 'neutral'], [1413, 'neutral'], [1414, 'neutral'], [1415, 'neutral'], [1416, 'neutral'], [1417, 'contradiction'], [1418, 'entailment'], [1419, 'entailment'], [1420, 'neutral'], [1421, 'neutral'], [1422, 'contradiction'], [1423, 'entailment'], [1424, 'contradiction'], [1425, 'entailment'], [1426, 'entailment'], [1427, 'neutral'], [1428, 'neutral'], [1429, 'neutral'], [1430, 'entailment'], [1431, 'neutral'], [1432, 'neutral'], [1433, 'entailment'], [1434, 'entailment'], [1435, 'neutral'], [1436, 'neutral'], [1437, 'entailment'], [1438, 'neutral'], [1439, 'contradiction'], [1440, 'neutral'], [1441, 'entailment'], [1442, 'contradiction'], [1443, 'entailment'], [1444, 'neutral'], [1445, 'entailment'], [1446, 'contradiction'], [1447, 'entailment'], [1448, 'contradiction'], [1449, 'entailment'], [1450, 'entailment'], [1451, 'neutral'], [1452, 'entailment'], [1453, 'neutral'], [1454, 'entailment'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'neutral'], [1458, 'neutral'], [1459, 'neutral'], [1460, 'contradiction'], [1461, 'neutral'], [1462, 'contradiction'], [1463, 'entailment'], [1464, 'entailment'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'neutral'], [1468, 'neutral'], [1469, 'contradiction'], [1470, 'entailment'], [1471, 'contradiction'], [1472, 'neutral'], [1473, 'contradiction'], [1474, 'contradiction'], [1475, 'contradiction'], [1476, 'neutral'], [1477, 'entailment'], [1478, 'entailment'], [1479, 'entailment'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'neutral'], [1484, 'entailment'], [1485, 'neutral'], [1486, 'neutral'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'neutral'], [1490, 'neutral'], [1491, 'entailment'], [1492, 'neutral'], [1493, 'neutral'], [1494, 'neutral'], [1495, 'entailment'], [1496, 'contradiction'], [1497, 'entailment'], [1498, 'entailment'], [1499, 'contradiction'], [1500, 'entailment'], [1501, 'neutral'], [1502, 'entailment'], [1503, 'entailment'], [1504, 'entailment'], [1505, 'entailment'], [1506, 'entailment'], [1507, 'entailment'], [1508, 'neutral'], [1509, 'contradiction'], [1510, 'entailment'], [1511, 'contradiction'], [1512, 'contradiction'], [1513, 'neutral'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'entailment'], [1517, 'neutral'], [1518, 'contradiction'], [1519, 'entailment'], [1520, 'entailment'], [1521, 'contradiction'], [1522, 'neutral'], [1523, 'entailment'], [1524, 'contradiction'], [1525, 'neutral'], [1526, 'neutral'], [1527, 'entailment'], [1528, 'neutral'], [1529, 'entailment'], [1530, 'neutral'], [1531, 'entailment'], [1532, 'entailment'], [1533, 'entailment'], [1534, 'neutral'], [1535, 'contradiction'], [1536, 'neutral'], [1537, 'entailment'], [1538, 'entailment'], [1539, 'neutral'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'neutral'], [1543, 'neutral'], [1544, 'entailment'], [1545, 'contradiction'], [1546, 'neutral'], [1547, 'neutral'], [1548, 'neutral'], [1549, 'neutral'], [1550, 'contradiction'], [1551, 'entailment'], [1552, 'neutral'], [1553, 'neutral'], [1554, 'neutral'], [1555, 'neutral'], [1556, 'entailment'], [1557, 'neutral'], [1558, 'entailment'], [1559, 'entailment'], [1560, 'neutral'], [1561, 'entailment'], [1562, 'neutral'], [1563, 'neutral'], [1564, 'neutral'], [1565, 'entailment'], [1566, 'contradiction'], [1567, 'entailment'], [1568, 'entailment'], [1569, 'contradiction'], [1570, 'neutral'], [1571, 'entailment'], [1572, 'entailment'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'neutral'], [1577, 'contradiction'], [1578, 'contradiction'], [1579, 'contradiction'], [1580, 'neutral'], [1581, 'contradiction'], [1582, 'neutral'], [1583, 'entailment'], [1584, 'neutral'], [1585, 'neutral'], [1586, 'neutral'], [1587, 'neutral'], [1588, 'entailment'], [1589, 'entailment'], [1590, 'neutral'], [1591, 'entailment'], [1592, 'contradiction'], [1593, 'neutral'], [1594, 'neutral'], [1595, 'neutral'], [1596, 'entailment'], [1597, 'entailment'], [1598, 'neutral'], [1599, 'neutral'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'neutral'], [1604, 'entailment'], [1605, 'neutral'], [1606, 'entailment'], [1607, 'entailment'], [1608, 'contradiction'], [1609, 'entailment'], [1610, 'neutral'], [1611, 'neutral'], [1612, 'entailment'], [1613, 'neutral'], [1614, 'entailment'], [1615, 'contradiction'], [1616, 'neutral'], [1617, 'neutral'], [1618, 'contradiction'], [1619, 'contradiction'], [1620, 'contradiction'], [1621, 'entailment'], [1622, 'contradiction'], [1623, 'neutral'], [1624, 'contradiction'], [1625, 'neutral'], [1626, 'neutral'], [1627, 'contradiction'], [1628, 'entailment'], [1629, 'contradiction'], [1630, 'contradiction'], [1631, 'entailment'], [1632, 'entailment'], [1633, 'entailment'], [1634, 'entailment'], [1635, 'neutral'], [1636, 'contradiction'], [1637, 'contradiction'], [1638, 'contradiction'], [1639, 'entailment'], [1640, 'entailment'], [1641, 'neutral'], [1642, 'entailment'], [1643, 'entailment'], [1644, 'contradiction'], [1645, 'neutral'], [1646, 'neutral'], [1647, 'contradiction'], [1648, 'entailment'], [1649, 'neutral'], [1650, 'entailment'], [1651, 'entailment'], [1652, 'neutral'], [1653, 'entailment'], [1654, 'contradiction'], [1655, 'entailment'], [1656, 'contradiction'], [1657, 'contradiction'], [1658, 'neutral'], [1659, 'entailment'], [1660, 'entailment'], [1661, 'neutral'], [1662, 'entailment'], [1663, 'neutral'], [1664, 'neutral'], [1665, 'neutral']]\n"
     ]
    }
   ],
   "source": [
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "answer = num_to_label(pred_answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1        neutral\n",
      "2         2        neutral\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661        neutral\n",
      "1662   1662     entailment\n",
      "1663   1663        neutral\n",
      "1664   1664        neutral\n",
      "1665   1665        neutral\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('./result/submission.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e17ea03fbb32132e5671c308166b4379813c9bd4e6a684633c3a9083728c29e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('stdata')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
