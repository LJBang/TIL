# 22.06.10

## MLP와 딥모델
시퀀스 형태의 데이터이기 때문에 자연스럽게 RNN, LSTM  
TextCNN도 나쁘지 않았다.  

그러나 Transformer계열 모델이 나오면서 지배했다.  
- BERT는 Transformer encoder를 쌓은 모델  
- GPT는 Transformer decoder를 쌓은 모델  

### RNN계열
인코더에서는 단어와 이전단어 인코드를 입력으로 새로운 히든 스테이트를 만든다.  
인코딩 과정이 끝나면 임베딩된 벡터는 디코더로 가고,  
들어온 단어에 따라서 다음 단어를 생성한다.  

단점  
- 앞쪽 단어의 의미가 뒤로 갈 쑤록 희석됨  
    -> 멀리 떨어진 단어들의 관계 정립이 어려움  
- 순차적인 속정으로 인해 느린 속도  

## Transformer 모델
attention으로만 단어의 의미를 문맥에 맞게 잘 표현할 수 있다.  
병렬화가 가능하다.  

시퀀스 입력에 대해서 인코더 - 디코더를 통과한 후 시퀀스 출력을 만든다.  

인코더와 디코더는 여러 층으로 구성되어 있다.  
각각의 인코더(디코더)층은 같은 구조로 이루어져 있으나 파라미터의 공유는 없다.  

### 인코더
인풋 문장이 들어왔을 때, 각각 단어의 임베딩(x1)이 있다고 한다면.  
각 단어의 임베딩이 self-attention층을 지나면서 다른 단어들과의 관계를 고려하여 새로운 임베딩(z1)가 생성된다.  
z1는 또 피드포워드 층을 통과한다. 하나의 피드포워드 층은 동일한 파라미터를 가지지만 각각의 단어가 독립적으로 들어간다.  
피드포워드 층을 지나면서 단어의 표현력을 더 높여준다.  
위 과정이 하나의 인코더 층에서 이뤄지는 과정이다.  

### self-attention
it과 같은 단어는 문맥에 의해서 결정된다.  
현재 단어의 의미를 주변 단어들의 의미의 조합(weighted sum)으로 표현한다.  
즉 각 단어가 가지는 embedding의 weighted sum이 현재 단어의 새로운 의미가 된다.  
이것의 장점은 현재 단어를 기준으로 양방향의 정보를 모두 반영할 수 있다는 것이다.  

#### 벡터를 기준으로
임베딩 된 입력단어 X1, X2를 생각해보자.  
이를 이용해 q1, k1, v1와 q2, k2, v2의 세 벡터들을 생성한다.(쿼리, 키, 밸류)  
`X1 * W^Q = q1`와 같이 입력 단어에 각각의 가중치 행렬을 곱하여 생성한다.  
각 가중치 행렬 W^Q, W^K, W^V가 모델의 파라미터이다.  
그렇다면 q, k, v벡터는 모델의 함수값이라고 할 수 있다.  

기본적으로 X의 사이즈는 512, q,k,v는 64정도로 사용한다.  

a_ij를 i번째 단어에 미치는 j번째 단어의 영향의 크기라 생각하자.  
dot(q1, k2)은 단어1이 단어2에 미치는 영향 점수. 입력된 단어 j개에 대해서 각각 수행한다.  
예를 들어 `dot(q1, k1) = 112`, `dot(q1, k2) = 96`라고 하자.  
이를 위에서 정한 k의 사이즈(=64)의 루트 값으로 나누어 준다. 이는 안정적인 결과를 내기 위함이다.  
그렇다면 112/8 = 14, 96/8 = 12가 되고, 이들을 softmax를 취해줘서 확률값으로 만들어준다.  
이렇게 만들어진 확률값들이 가중치 행렬인 attention행렬의 i번째 값 a_i의 값이 된다.  
이 확률값, 즉 weight_j을 value_j와 곱해주면 단어 j의 i에 대한 의미 가중치가 되고 모든 j에 대해서 이를 더해주면 i의 의미가 새로 정해지고 zi가 된다.  

### 텐서를 기준으로
위에선 벡터로 표현했지만 batch를 생각한다면 행렬, tensor로 연산하게 된다.  
하나의 단어는 x1이지만 이를 합친 X는 연속된 단어들을 표현하는 모양이다.  
따라서 `X*WQ = Q`와 같이 각 단어별로 진행하던 계산을 묶을 수 있다.  
이 과정을 통해서 Q, K, V행렬을 만들 수 있다.  

---   
막간 용어 정리  
- s: sequence length, 입력 단어의 개수  
- h: size per head, W행렬의 열의 개수(64)  
- d: input embedding size, 단어 벡터의 크기(512)  

이를 따르면 `X=(s*d)크기의 행렬`, `W=(d*h)크기의 행렬`
---  
ATT행렬은 `softmax( (Q * KT) / sqrt(d_k) )`가 되고,  
`softmax( (Q * KT) / sqrt(d_k) ) * V = Z`가 된다.  
이 때 Z 행렬은 (s*h)크기

### Multi headed attention
위에서 구한 것은 헤드가 하나인 것.  
하나의 어텐션 매트릭스로는 이상적인 값이 아닐 수 있다.  
다양한 어텐션 매트릭스를 구해서 성능을 올리는 것.  

헤드별로 Q,K,V 행렬을 구하기 위한 가중치 행렬 WQ, WK, WV가 있다.  
(W1Q, W1K, W1V, W2Q, W2K, W2V, ... )  

그렇다면 어텐션 헤드 하나 당 Z행렬이 생성된다.  
이렇게 생성된 n개의 Z행렬들을 수평적으로 concate.  
이것과 W^O를 곱하여 (s*d)크기의 행렬 Z를 만드는 것이 목표.  

위 과정에서 우리는 W를 학습해야 한다.  

