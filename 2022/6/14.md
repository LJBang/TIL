# 22.06.14

## 인공지능 개념 준비

### 배치
GD에서 기울기를 한 번 업데이트 할 때마다 전체 학습 데이터에 대해서 기울기를 구하고, 모델 파라미터를 업데이트 한다.  
-> batch를 나누어서 학습하자 -> SGD, stochastic gradient descent  

SGD는 기울기를 한 번 업데이트 하기 위해서 batch만큼의 데이터를 사용한다.  
한 에포크당 여러개의 배치가 학습된다.  

이렇게 배치단위로 학습을 할 경우 FC-activate 층을 지나면서 배치간에 데이터 분포의 차이가 생길 수 있다. -> internal covariant shift  

batch normalization은 각 배치별로 데이터를 정규화 해서 분포가 달라지더라도 대응할 수 있다.  

#### 학습단계의 배치 정규화
학습단계에서는 BN이 배치별로 진행되어야 한다. 그래야지 각 배치들이 표준 정규 분포를 따르게 되기 때문.  
정규화를 해주면 각 Feature들이 동일한 크기로 scale되기 때문에 learning rate결정에 유리  

#### 추론 단계의 배치 정규화
추론 과정에서는 BN에 적용할 평균과 분산을고정한다.  
이 때 사용하는 고정된 평균과 분산은 학습시 사용한 데이터의 이동평균 혹은 지수평균을 사용한다.  

## scheduled sampling
RNN계열의 auto-regressive모델에서 train단계에서는 정답 단어들을 가지고 학습하지만,  
추론(inference)단계에서는 이전 단어를 가지고 새 단어를 생성한다.  
이때, 이전 단어가 잘못되면 영영 잘못되기 때문에 간극을 줄일 필요가 있다.  

이를 위해서 학습할(train) 때 다음 단어를 예측하는 과정에서 정답 단어를 사용할 지, 예측된 단어를 사용할 지를 확률적으로 선택하는 방식이다.  

학습 초기에는 모델의 수렴을 위해서 정답 비율($\epsilon_i$)을 올리고, 점차 추론단계와 비슷하게 맞추기 위해서 예측된 단어 비율을 올리는 방법을 사용한다.  

이때 입실론을 어떻게 변화시킬지를 결정해야 하는데 세 가지 함수가 있다.  
- 선형 감쇠
- inverse sigmoid 감쇠
- 지수 감쇠  

