# 22.06.13
## Transformer
### positional encoder
단어의 위치 정보를 저장할 필요가 있다.  
인코더의 input으로 포지셔널 임베딩과 단어의 기존 임베딩을 더해서 만든다.  

포지셔널 임베딩을 따로 학습하는 것이 아니라, 그저 특정 함수를 사용해서 만든다.  

### residual connection
인코더 내에서 self-attention의 결과와 input을 더해주고 normalization 적용 후 feed forward층에 넣어주고 또 그 결과와 feeo forward층의 인풋을 더해준 후 nomalize하고 결과를 만든다.  

layer가 깊어질수록 vanishing gradient문제를 포함한 다양한 문제가 발생할 수 있는데, residual 방식은 얕은 모델의 장점을 이용해 그러한 문제들의 영향을 줄일 수 있다.  

### Decoder
인코더에서 최종적으로 tensor를 출력하는데, 이를 가지고 K, V행렬을 구한다.  
Q는 필요가 없는데 이는 디코더에서 번역된 단어들이 Q의 역할을 한다.  

시작토큰과 K, V를 이용해 첫 단어를, 첫 단어와 K, V를 이용해 두번째 단어를 예측하고 출력한다. 마지막에 종료 토큰이 출력되면 문장 추론이 끝난다.  

디코더에서 출력된 임베딩 벡터는 linear층을 거쳐 단어 벡터의 size만큼 바뀌고,  
softmax를 통과한 후 확률적으로 가장 큰 값을 가지는 index가 해당 단어의 임베딩된 index이다.  

### 학습 시 Error function
crossEntropy 사용  
`target = [0, 0, 0, 1, 0, 0]`, `y = [0.2, 0.2, 0.1, 0.2, 0.2, 0.1]`이라 하자.  
이 경우 크로스 엔트로피는 $-\sum{t_i ln{y_i}} = -ln{0.2} = 1.69$가 된다.  
y값이 target에 가까워진다면 크로스엔트로피는 점점 작아질 것이므로 이를 위해서 학습한다.  
