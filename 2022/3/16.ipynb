{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "### Subword Tokenization\n",
    "BPE: 단어를 철자별로 분리한 후 연속한 철자쌍 중 빈도수가 높은 것들을 하나의 토큰으로 묶는다. 이를 반복한다.  \n",
    "Wordpiece: BERT에 주로 사용, 기호의 쌍을 묶을 때, 빈도수 대신 likelihood를 최대화 시키는 쌍을 찾는다.  \n",
    "\n",
    "### 단어 정규화\n",
    "단어들 사이의 유사성을 이해해야 하기 때문에 단어들을 정규화 해주어야 하나,  \n",
    "최근 단어 임베딩을 사용해 단어를 임베딩 시키면서 비슷한 단어는 비슷한 벡터로 묶이게 되므로 정규화의 필요성이 줄어들고 있다.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
