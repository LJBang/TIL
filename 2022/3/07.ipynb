{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.03.07\n",
    "\n",
    "## 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 전처리  \n",
    "\n",
    "#### 토큰화\n",
    "텍스트를 토큰 단위로 자르는 것. 단어 토큰화와 문장 토큰화가 있다.  \n",
    "주의할 점은 구두점을 모두 떼는 것이 아니라 적절하게 활용할 수 있도록 해야한다. 조사들이 많이 붙어있기 때문에 영어보다 한국어 토큰화가 더 어렵다.  \n",
    "`NLTK`의 `word_tokenize`나 `WordPunctTokenizer`등 기존에 만들어진 규칙이 내가 사용하려는 규칙과 맞다면 사용하면 편하다.  \n",
    "한국어 문장 토큰화 규칙의 경우 `KSS`가 있다.  \n",
    "\n",
    "#### 한국어 토큰화  \n",
    "`KoNLPy`를 사용해서 한국어 문장에서 단어를 토큰화 할 수 있다.  \n",
    "규칙의 종류는 Okt, Mecab, Komoran, Hannanum, Kkma이 있다.  \n",
    "\n",
    "#### 정제와 정규화  \n",
    "정제는 텍스트 코퍼스에서 노이즈를 제거하고, 정규화는 다르게 표현된 단어들을 통합시키는 작업.  \n",
    "대소문자를 통합하고, 불필요한 단어를 제거하는 경우가 있다.\n",
    "\n",
    "#### 표제어 추출  \n",
    "am, are, is는 be라는 원형을 갖는다. 이때 be를 표제어(Lemma)라고 한다.  \n",
    "이는 `NLTK`의 `WordNetLemmatizer`를 사용하면 영어 단어에서 표제어를 추출해준다.  \n",
    "이를 진행하면 품사의 정보를 보존하면서 표제어를 추출한다.  \n",
    "\n",
    "#### 어간 추출\n",
    "표제어 추출의 가장 섬세한 방법은 어간(Stem)과 접사로 나누는 방법이다.  \n",
    "sounding에서 sound를 추출하는 작업을 어간 추출이라고 하는데, 규칙에 기반한 작업이기 때문에 dying을 dy로 바꾸는 등 사전에 없는 단어로도 바꿀 수 있다.  \n",
    "\n",
    "#### 불용어 처리\n",
    "큰 의미가 없는 단어 토큰을 불용어(Stopword)라고 한다. 이를 제거하면 의미가있는 단어들만 남게 된다.  \n",
    "`NLTK`에서는 `corpus`의 `stopword`를 사용할 수 있다.  \n",
    "\n",
    "#### 인코딩  \n",
    "단어들을 학습에 사용하기 위해서는 숫자형으로 바꿔줄 필요가 있다.  \n",
    "각 단어에 `cat=1, dog=2, eat=3, ...`과 같이 각 단어에 정수를 매기는 라벨 인코딩과,  \n",
    "`cat=[1,0,0], dog=[0,1,0], eat=[0,0,1]`과 같이 원-핫 벡터를 매기는 원-핫 인코딩이 있다.  \n",
    "라벨 인코딩의 경우 학습할 때, 라벨값, 즉 수치에 영향을 받을 수 있다는 단점이 있고,  \n",
    "원학 인코딩의 경우 단어의 개수가 늘어날수록 필요한 공간이 계속 늘어난다는 단점이 있다.  \n",
    "\n",
    "#### 패딩\n",
    "자연어에서는 각 문장의 길이가 다른 경우가 많다.  \n",
    "문서에서 모든 문장의 길이를 맞춰 준다면 병렬 연산을 통해서 더 빠르게 처리할 수 있다.  \n",
    "예를 들어 단어들이 라벨 인코딩된 두 문장 `[3, 4, 1], [2, 4, 5, 8]`이 있을 때, 가장 긴 문장을 기준으로 패딩을 적용한다면, `[3, 4, 1, 0], [2, 4, 5, 8]`과 같이 문장의 길이를 맞춰줄 수 있다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어 모델\n",
    "\n",
    "언어 모델은 단어 시퀀스에 확률을 할당하는 일. 즉 가장 자연스러운단어 시퀀스를 찾는 모델이다.  \n",
    "가장 보편적인 방법은 이전 단어들을 사용해 다음 단어를 예측하는 것.  \n",
    "조건부 확률을 이용해 이전의 단어들이 나왔을 때, 다음 단어가 등장할 확률을 계산하여 곱하고 문장의 확률값을 뽑아낸다.  \n",
    "  \n",
    "![seq](./image/seq.png)  \n",
    "\n",
    "그러나 위 문장이 실제로 사용될 수 있음에도 불구하고 학습데이터에서는 없을 수도 있다.  \n",
    "`n-gram`의 경우 $P(is|An adorable little boy)와 $P(is|boy)$의 확률을 유사하다고 생각하고 단어들을 n개의 뭉치로 나누어 이를 토큰으로 사용하는 방법이다.  \n",
    "이는 희소성 문제에는 적용할 수 있지만 전체 문장을 이용한 모델보다는 정확도가 떨어진다.  \n",
    "\n",
    "언어모델은 시퀀스를 찾는 모델이지만 한국어의 경우 순서가 섞여도 모두 문장이 되는, 시퀀스로부터 비교적 자유로운 언어이기 때문에 다음 단어를 예측하는 것이 어렵다.  \n",
    "\n",
    "### PPL, perplexity\n",
    "테스트 데이터를 이용해 언어모델을 평가해볼 수 있다. PPL은 언어 모델을 평가하는 지표이다.  \n",
    "PPL값이 낮을수록 좋다. 문장의 길이로 정규화된 문장 확률의 역수라고 보면 된다.  \n",
    "  \n",
    "![PPL](./image/ppl.png)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카운트 기반의 단어 표현\n",
    "#### Bag of Words\n",
    "문장 내에서 각 단어의 출현빈도에만 집중하는 텍스트 수치화 방법.  \n",
    "단어의 출현 빈도만 고려하기 때문에 자주 출현하는 단어에 따라서 문서의 분류나 유사도 문제에 적용할 수 있다.  \n",
    "\n",
    "#### 문서 단어 행렬\n",
    "여러 문서에 대해서 각 단어들이 나온 빈도를 행렬로 표현한 것  \n",
    "\n",
    "![dtm](./image/dtm.png)  \n",
    "\n",
    "서로 다른 문서르 비교하기 용이하나, 공간을 더 많이 차지하는 희소 표현 문제가 발생할 수 있다.  \n",
    "단순 빈도 기반이기 때문에 쓸모없는 단어가 메인이 될 수 있다.  \n",
    "\n",
    "#### TF-IDF\n",
    "`tf`는 한 문서에서 특정 단어가 등장하는 빈도수  \n",
    "`df`는 한 단어가 등장하는 문서의 수  \n",
    "`idf`는 `df`의 역수의 로그 값  \n",
    "`tf-idf`는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하고, 특정 문서에서 자주 등장하는 단어는 중요도가 높다고 판단한다. 단어의 `tf-idf`값이 크면 중요도가 높은 단어이다.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
