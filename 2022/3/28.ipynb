{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22.03.28\n",
    "## 데브코스\n",
    "### Transformer\n",
    "BERT는 Transformer의 encoder를 사용, GPT는 decoder를 사용  \n",
    "\n",
    "#### 기존 RNN\n",
    "인코더의 입력된 문자열 시퀀스와 이전 상태를 입력으로 받아서 디코더에 넘겨준다.  \n",
    "디코더는 이를 입력 받고 단어 시퀀스를 출력한다.  \n",
    "이는 멀리떨어진 단어들 간 의존성 모델링이 힘들고, 순차적이기 때문에 느리다.  \n",
    "\n",
    "#### Text CNN\n",
    "단어를 임베딩하고 이를 수직적으로 붙여서 1차원인 텍스트를 2차원 행렬로 바꿔주고 CNN을 적용한다.  \n",
    "문장에 대해서 크기가 다른 여러개의 필터를 각각 적용하여 피쳐맵을 만든다.  \n",
    "이를 맥스풀링 해서 특징을 추린다.  \n",
    "마지막으로 소프트맥스 함수를 적용해 분류한다.  \n",
    "\n",
    "#### Transformer\n",
    "트랜스포머가 풀려고 하는 문제  \n",
    "- seq2seq\n",
    "- 순차입력에 대해 순차출력을 반환\n",
    "- 기계번역\n",
    "- 질의응답  \n",
    "\n",
    "인코더와 디코더는 여러개의 층으로 이루어져 있고, 각각의 인코더의 결과는 다음 인코더 하나와 모든 디코더로 전달된다.  \n",
    "\n",
    "각각의 임베딩 된 단어가 셀프 어텐션 층을 지나면서 각 단어와 주변 단어의 의미를 종합해 문맥화 된 의미를 가지게 된다. 이를 또 피드포워드 신경망에 넣어서 의미를 강화한다.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
