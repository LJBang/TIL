# 21.12.30

## 데브코스
4주 4일차  

### 모델 배포
모델을 단순히 하드코딩 형태로 서빙하기 보다는  
핸들러를 만들어서 각각의 feature를 나누어주는 것이 좋다.  
모델의 serving을 위해서는 보통 tensorflow serving, torch serve를 사용하고,  
클라이언트와 소통하기 위해서 Flask나 웹 프레임워크를 사용한다.  
모델을 저장하는 Serialize과정과 de-Serialize과정에서 같은 방법을 사용해야 한다.  
예를 들어 joblib모듈을 이용해 모델을 pkl형태로 만들었을 경우 jopblib으로 풀어줘야하지 pickle모듈로 풀어준다면 오류가 발생한다.  

인스턴스는 aws ec2인스턴스에 deep learning AMI를 사용했고,  
패키지와 가상환경이 기본적으로 깔려있는 것이 많다.  

모델을 저장할 때 joblib모듈을 사용했는데, pickle을 사용할 경우 파일을 바이너리 모드로 열고 저장해야하므로 조금 번거롭다.  
joblib의 경우 좀 더 간단하게 모델을 저장할 수 있다.  

### modelHandler  

```py
class ModelHandler(BaseHandler):
    def __init__(self):
        pass

    def initialize(self, **kwarg):
        # 데이터 처리나 모델 초기화(불러오기)
        pass

    def preprocess(self, data):
        # 데이터 전처리와
        # 데이터를 모델의 input에 맞게 가공
        pass

    def inference(self, data):
        # 입력값에 대한 예측, 추론
        pass

    def postprocess(self, data):
        # 예측된 결과에 대한 후처리
        # 보통 확률분포를 인식할 수 있는 정보로 변환
        pass

    def handle(self, data):
        # 요청 정보를 받아 적절한 응답을 반환
        # 데이터 입력 확인
        # 데이터 전처리
        # 모델 추론
        # 반환값의 후처리
        # 결과
        pass
```  
이렇게 각 기능별로 함수를 만들면 훨씬 구조적이고, 유지보수하기도 쉽다.  


