# 21.08.31

## BERT 
일단 홍직이가 사용한 것들부터 분석  
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW

# AutoTokenizer
model_checkpoint = 'google/bert_uncased_L-12_H-512_A-8'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

# AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

args = TrainingArguments(
    "test-ynat",
    evaluation_strategy="steps",
    learning_rate=3e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=2,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
)
```  

`AutoTokenizer`  
`from_pretrained` 함수에서 파라미터로 받은 모델 이름에 따라서 토크나이저를 선택해서 받아옴  
`model_checkpoint`에서 `bert`를 받아오고 있음.  

`AutoModelForSequenceClassification`  
`from_pretrained`함수에서 파라미터로 받은 모델 이름에 따라서 분류 모델을 정함.  

`TrainingArguments`  
위에서 정한 모델에 넣어줄 하이퍼파라미터들을 정함.  

`Trainer`  
모델, 하이퍼파라미터, 트레인셋, 밸리데이션 셋, 토크나이저 순으로 대입하여 학습준비.  
`Trainer.train()`을 통해 학습 시작  

pytorch기반으로 만들어진 것인가봄.