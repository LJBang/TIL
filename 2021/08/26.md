# 21.08.26

## BERT
구글에서 만든 `사전훈련 언어모델`  
자연어를 학습하기 전에 임베딩 과정에서 BERT를 사용한다.  
전체 임베딩 과정은 세 단계를 거친다.  

### Token Enbedding
각 Char에서 시작해 자주 등장한 Sub-Word조합을 붙여나가면서 하나의 단위로 만듦.  
최종적으로는 각 단어들을 Sub-Word의 조합으로 만들어 토큰화.  
BPE와 유사한 과정을 거쳐서 토크나이징 함.

### Segment Embedding
토큰으로 이루어진 입력된 두 개의 문장이 이어지는지 아닌지 확인  
두개의 문장을 구분자 `[SEP]`를 넣어 구분하고, 두 문장을 하나의 `Segment`로 지정  
한 세그먼트를 이루는 Sub-Word는 최대 512개.  
한 문장에서 단어들을 마스킹해서 모델이 예측하게 하고,  
예측을 좀 더 정확하게 하기 위해서 두개의 문장의 문맥이 이어지는지 확인.  

### Position Embedding
RNN에서는 입력에 따라 단어의 위치를 알 수 있지만, BERT에서는 알 수 없음.  
따로 단어의 위치를 알 수 있는 행렬을 만들어 단어의 위치를 제공.  
단어의 위치는 `sin`, `cos`함수를 이용한다.  

### BERT
이미 3.3억개의 코퍼스를 정제, 임베딩하여 학습시킨 사전훈련 모델이다.  
